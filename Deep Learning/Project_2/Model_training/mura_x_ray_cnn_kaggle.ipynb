{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Dropout, Flatten, MaxPool2D, BatchNormalization # Layers to be used for building our model\n",
    "from tensorflow.keras.models import Model # The class used to create a model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from typing import List, Dict, Optional, Tuple, Any, Union\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-03-26T16:00:45.204271Z",
     "iopub.execute_input": "2023-03-26T16:00:45.204688Z",
     "iopub.status.idle": "2023-03-26T16:00:45.214831Z",
     "shell.execute_reply.started": "2023-03-26T16:00:45.204630Z",
     "shell.execute_reply": "2023-03-26T16:00:45.213706Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# NEW on TPU in TensorFlow 24: shorter cross-compatible TPU/GPU/multi-GPU/cluster-GPU detection code\n",
    "\n",
    "try: # detect TPUs\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError: # detect GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "    #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n",
    "\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:00:45.820283Z",
     "iopub.execute_input": "2023-03-26T16:00:45.820672Z",
     "iopub.status.idle": "2023-03-26T16:00:45.833518Z",
     "shell.execute_reply.started": "2023-03-26T16:00:45.820639Z",
     "shell.execute_reply": "2023-03-26T16:00:45.832347Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "Num GPUs Available:  2\nNumber of accelerators:  2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Globals"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-23T20:53:30.840859Z",
     "iopub.execute_input": "2023-03-23T20:53:30.841911Z",
     "iopub.status.idle": "2023-03-23T20:53:30.847193Z",
     "shell.execute_reply.started": "2023-03-23T20:53:30.841859Z",
     "shell.execute_reply": "2023-03-23T20:53:30.845886Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# define path under which MURA-v1.1/ is located:\n",
    "print(os.getcwd())\n",
    "root_path: str = \"/kaggle/input/mura-x-ray-dataset/\"\n",
    "os.chdir(root_path)\n",
    "os.getcwd()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:00:48.769583Z",
     "iopub.execute_input": "2023-03-26T16:00:48.770300Z",
     "iopub.status.idle": "2023-03-26T16:00:48.782909Z",
     "shell.execute_reply.started": "2023-03-26T16:00:48.770261Z",
     "shell.execute_reply": "2023-03-26T16:00:48.781844Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/input/mura-x-ray-dataset\n",
     "output_type": "stream"
    },
    {
     "execution_count": 6,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'/kaggle/input/mura-x-ray-dataset'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weights & Biases\n",
    "\n",
    "In order to make experiment tracking easier we will use [Weights & Biases](wandb.ai/home), which offers a free lisence for academic purposes. For the sake of this assignment a team has been created:https://wandb.ai/aueb. Access can be granted by contacting the authors.\n",
    "\n",
    "* Note that this code assumes that you have already set up a Wandb account and API key. If you haven't done so yet, you will need to sign up for a free account at https://wandb.ai/ and follow the instructions there to obtain your API key.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:00:50.281688Z",
     "iopub.execute_input": "2023-03-26T16:00:50.283973Z",
     "iopub.status.idle": "2023-03-26T16:00:55.266880Z",
     "shell.execute_reply.started": "2023-03-26T16:00:50.283921Z",
     "shell.execute_reply": "2023-03-26T16:00:55.265675Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path /kaggle/input/mura-x-ray-dataset/wandb/ wasn't writable, using system temp directory.\nwandb: WARNING Path /kaggle/input/mura-x-ray-dataset/wandb/ wasn't writable, using system temp directory\n\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path /kaggle/input/mura-x-ray-dataset/wandb/ wasn't writable, using system temp directory\n\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
     "output_type": "stream"
    },
    {
     "output_type": "stream",
     "name": "stdin",
     "text": "  ········································\n"
    },
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
     "output_type": "stream"
    },
    {
     "execution_count": 7,
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data\n",
    "\n",
    "Data has the following structure:\n",
    "\n",
    "```\n",
    "├──MURA\n",
    "  ├──train_image_paths.csv\n",
    "  ├──train_labeled_studies.csv\n",
    "  ├──valid_image_paths.csv\n",
    "  ├──valid_labeled_studies.csv\n",
    "  ├──train\n",
    "  │   └─ BODY PART\n",
    "  │       └─ patientxxx\n",
    "  │          .\n",
    "  │          .\n",
    "  │          .\n",
    "  │    .\n",
    "  │    .\n",
    "  │    .\n",
    "  └──test\n",
    "      └─ BODY PART\n",
    "          └─ patientxxx\n",
    "             .\n",
    "             .\n",
    "             .\n",
    "       .\n",
    "       .\n",
    "       .  \n",
    "```\n",
    "\n",
    "We will create a dataframe that uses these paths as rows and extract any information needed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_set_category(string: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the 'set_type' and 'category' from a given string using regular expressions.\n",
    "\n",
    "    Parameters:\n",
    "        string (str): A string containing the 'set' and 'category' information.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the 'set' and 'category' information.\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r\".*(?P<set_type>train|valid)/(?P<category>XR_[A-Z]+)/(?P<patient_id>patient\\d+)/study.*\"\n",
    "    match = re.match(pattern, string)\n",
    "    if match:\n",
    "        return {'set_type': match.group('set_type'), 'category': match.group('category'), 'patient_id': match.group('patient_id')}\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def generate_path_df(dataset_type: str, dataset_path: str = \"/kaggle/input/mura-x-ray-dataset/\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads in the image paths and labels for a given dataset type (train or valid) from the MURA dataset.\n",
    "    Returns a pandas DataFrame containing the image paths, labels, and the dataset type.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset_type (str): The type of dataset to read in (train or valid)\n",
    "        dataset_path (str): The path to the MURA dataset folder (default: '/kaggle/input/mura-x-ray-dataset/')\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the image paths, labels, and dataset type.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Read in the image paths csv file and assign the column name 'image_path'\n",
    "    train_label_paths = pd.read_csv(f\"{dataset_path}/MURA-v1.1/{dataset_type}_image_paths.csv\", header=None, names=['image_path'])\n",
    "    \n",
    "    # Extract the path to the folder containing the image file and create a new column 'path'\n",
    "    train_label_paths[\"path\"] = train_label_paths.apply(lambda x: \"/\".join(x['image_path'].split(\"/\")[:-1]) + \"/\", axis=1)\n",
    "    \n",
    "    # Read in the labeled studies csv file and assign column names 'path' and 'label'\n",
    "    train_labels = pd.read_csv(f\"{dataset_path}/MURA-v1.1/{dataset_type}_labeled_studies.csv\", header=None, names=['path', 'label'])\n",
    "    \n",
    "    # Merge the two DataFrames on the 'path' column and create a new column 'image_type'\n",
    "    _df = train_labels.merge(train_label_paths, on='path', how='left')\n",
    "    \n",
    "    # Check that the length of the two DataFrames match\n",
    "    assert len(train_label_paths) == len(_df)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def generate_dataframes(dataset_path: str = \"/kaggle/input/mura-x-ray-dataset/\") -> pd.DataFrame:\n",
    "    \"\"\"Perfoms actions needed to load the dataset with image paths and additional info\"\"\"\n",
    "    \n",
    "    # read train test_dataframe\n",
    "    train: pd.DataFrame = generate_path_df(dataset_type=\"train\")\n",
    "    test: pd.DataFrame = generate_path_df(dataset_type=\"valid\")\n",
    "\n",
    "    # join dataframes\n",
    "    _df = pd.concat([train, test]).reset_index()\n",
    "\n",
    "    # Apply the extract_set_category function to each row of the DataFrame.\n",
    "    _df = pd.concat([_df, pd.DataFrame(_df['path'].apply(lambda x: extract_set_category(x)).tolist())], axis=1)\n",
    "    mapping: dict = {1: \"abnormal\", 0: \"normal\"}\n",
    "    _df['label_type'] = _df['label'].apply(lambda x: mapping[x])\n",
    "    \n",
    "    \n",
    "    # re-order columns\n",
    "    cols = list(_df.columns)\n",
    "    cols.remove(\"label\")\n",
    "    cols.append(\"label\")\n",
    "    _df = _df[cols]\n",
    "    \n",
    "    _df.drop([\"index\"], axis=1, inplace=True)\n",
    "    \n",
    "    return _df"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:01:00.483575Z",
     "iopub.execute_input": "2023-03-26T16:01:00.483990Z",
     "iopub.status.idle": "2023-03-26T16:01:00.498484Z",
     "shell.execute_reply.started": "2023-03-26T16:01:00.483951Z",
     "shell.execute_reply": "2023-03-26T16:01:00.497341Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = generate_dataframes()\n",
    "data.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:01:00.689019Z",
     "iopub.execute_input": "2023-03-26T16:01:00.689673Z",
     "iopub.status.idle": "2023-03-26T16:01:01.464761Z",
     "shell.execute_reply.started": "2023-03-26T16:01:00.689628Z",
     "shell.execute_reply": "2023-03-26T16:01:01.463471Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                path  \\\n0  MURA-v1.1/train/XR_SHOULDER/patient00001/study...   \n1  MURA-v1.1/train/XR_SHOULDER/patient00001/study...   \n2  MURA-v1.1/train/XR_SHOULDER/patient00001/study...   \n3  MURA-v1.1/train/XR_SHOULDER/patient00002/study...   \n4  MURA-v1.1/train/XR_SHOULDER/patient00002/study...   \n\n                                          image_path set_type     category  \\\n0  MURA-v1.1/train/XR_SHOULDER/patient00001/study...    train  XR_SHOULDER   \n1  MURA-v1.1/train/XR_SHOULDER/patient00001/study...    train  XR_SHOULDER   \n2  MURA-v1.1/train/XR_SHOULDER/patient00001/study...    train  XR_SHOULDER   \n3  MURA-v1.1/train/XR_SHOULDER/patient00002/study...    train  XR_SHOULDER   \n4  MURA-v1.1/train/XR_SHOULDER/patient00002/study...    train  XR_SHOULDER   \n\n     patient_id label_type  label  \n0  patient00001   abnormal      1  \n1  patient00001   abnormal      1  \n2  patient00001   abnormal      1  \n3  patient00002   abnormal      1  \n4  patient00002   abnormal      1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>image_path</th>\n      <th>set_type</th>\n      <th>category</th>\n      <th>patient_id</th>\n      <th>label_type</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00001/study...</td>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00001/study...</td>\n      <td>train</td>\n      <td>XR_SHOULDER</td>\n      <td>patient00001</td>\n      <td>abnormal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00001/study...</td>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00001/study...</td>\n      <td>train</td>\n      <td>XR_SHOULDER</td>\n      <td>patient00001</td>\n      <td>abnormal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00001/study...</td>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00001/study...</td>\n      <td>train</td>\n      <td>XR_SHOULDER</td>\n      <td>patient00001</td>\n      <td>abnormal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n      <td>train</td>\n      <td>XR_SHOULDER</td>\n      <td>patient00002</td>\n      <td>abnormal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n      <td>train</td>\n      <td>XR_SHOULDER</td>\n      <td>patient00002</td>\n      <td>abnormal</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Dev Test (valid) split\n",
    "\n",
    "* in order not to have any dependencies on the order of the data, we will shuffle the data. Moreover, 10% of the input data will be used as validation and 10% as test.\n",
    "\n",
    "As described in the Paper we will make sure not to have overlap between patients in the various sets. We shuffle the patients, and then split the dataset  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# take unique patient_ids and shuffle\n",
    "patients = data.patient_id.unique()\n",
    "np.random.seed(41)\n",
    "np.random.shuffle(patients)\n",
    "\n",
    "# split three list of patient_ids\n",
    "length_80 = patients[:int(len(patients)*0.8)]\n",
    "length_80_90 = patients[int(len(patients)*0.8):int(len(patients)*0.9)]\n",
    "length_90_100 = patients[int(len(patients)*0.9):]\n",
    "\n",
    "# sanity check\n",
    "data.loc[data.patient_id.isin(length_80)].describe()\\\n",
    ".join(data.loc[data.patient_id.isin(length_80_90)].describe(),rsuffix=\"_Validation\")\\\n",
    ".join(data.loc[data.patient_id.isin(length_90_100)].describe(), rsuffix=\"_Test\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:01:32.523371Z",
     "iopub.execute_input": "2023-03-26T16:01:32.523817Z",
     "iopub.status.idle": "2023-03-26T16:01:32.587396Z",
     "shell.execute_reply.started": "2023-03-26T16:01:32.523778Z",
     "shell.execute_reply": "2023-03-26T16:01:32.585627Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "              label  label_Validation   label_Test\ncount  31956.000000       4068.000000  3981.000000\nmean       0.412192          0.393314     0.409696\nstd        0.492237          0.488545     0.491839\nmin        0.000000          0.000000     0.000000\n25%        0.000000          0.000000     0.000000\n50%        0.000000          0.000000     0.000000\n75%        1.000000          1.000000     1.000000\nmax        1.000000          1.000000     1.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>label_Validation</th>\n      <th>label_Test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>31956.000000</td>\n      <td>4068.000000</td>\n      <td>3981.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.412192</td>\n      <td>0.393314</td>\n      <td>0.409696</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.492237</td>\n      <td>0.488545</td>\n      <td>0.491839</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Update the labels for each set & save dataframes to variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# update labels\n",
    "data.loc[data.patient_id.isin(length_80), \"set_type\"] = \"train\"\n",
    "data.loc[data.patient_id.isin(length_80_90), \"set_type\"] = \"validation\"\n",
    "data.loc[data.patient_id.isin(length_90_100), \"set_type\"] = \"test\"\n",
    "\n",
    "# Convert to sstring\n",
    "data[\"label\"] = data[\"label\"].astype(str)\n",
    "\n",
    "train: pd.DataFrame = data.loc[data.set_type == 'train']\n",
    "valid: pd.DataFrame = data.loc[data.set_type == 'validation']\n",
    "test: pd.DataFrame = data.loc[data.set_type == 'test']\n",
    "    \n",
    "train.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:01:36.120952Z",
     "iopub.execute_input": "2023-03-26T16:01:36.121933Z",
     "iopub.status.idle": "2023-03-26T16:01:36.188933Z",
     "shell.execute_reply.started": "2023-03-26T16:01:36.121894Z",
     "shell.execute_reply": "2023-03-26T16:01:36.187730Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                path  \\\n3  MURA-v1.1/train/XR_SHOULDER/patient00002/study...   \n4  MURA-v1.1/train/XR_SHOULDER/patient00002/study...   \n5  MURA-v1.1/train/XR_SHOULDER/patient00002/study...   \n6  MURA-v1.1/train/XR_SHOULDER/patient00003/study...   \n7  MURA-v1.1/train/XR_SHOULDER/patient00003/study...   \n\n                                          image_path set_type     category  \\\n3  MURA-v1.1/train/XR_SHOULDER/patient00002/study...    train  XR_SHOULDER   \n4  MURA-v1.1/train/XR_SHOULDER/patient00002/study...    train  XR_SHOULDER   \n5  MURA-v1.1/train/XR_SHOULDER/patient00002/study...    train  XR_SHOULDER   \n6  MURA-v1.1/train/XR_SHOULDER/patient00003/study...    train  XR_SHOULDER   \n7  MURA-v1.1/train/XR_SHOULDER/patient00003/study...    train  XR_SHOULDER   \n\n     patient_id label_type label  \n3  patient00002   abnormal     1  \n4  patient00002   abnormal     1  \n5  patient00002   abnormal     1  \n6  patient00003   abnormal     1  \n7  patient00003   abnormal     1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>image_path</th>\n      <th>set_type</th>\n      <th>category</th>\n      <th>patient_id</th>\n      <th>label_type</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n      <td>train</td>\n      <td>XR_SHOULDER</td>\n      <td>patient00002</td>\n      <td>abnormal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n      <td>train</td>\n      <td>XR_SHOULDER</td>\n      <td>patient00002</td>\n      <td>abnormal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00002/study...</td>\n      <td>train</td>\n      <td>XR_SHOULDER</td>\n      <td>patient00002</td>\n      <td>abnormal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00003/study...</td>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00003/study...</td>\n      <td>train</td>\n      <td>XR_SHOULDER</td>\n      <td>patient00003</td>\n      <td>abnormal</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00003/study...</td>\n      <td>MURA-v1.1/train/XR_SHOULDER/patient00003/study...</td>\n      <td>train</td>\n      <td>XR_SHOULDER</td>\n      <td>patient00003</td>\n      <td>abnormal</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Augmentation\n",
    "\n",
    "* As mentioned in the introductions, some pre-processing steps will take place, similar to the ones used in the paper."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import ImageEnhance, Image\n",
    "# Define the preprocessing function\n",
    "def normalize(x):\n",
    "    x /= 255.0  # Scale pixel values to [0, 1]\n",
    "    \n",
    "    # normalize to imagenet mean and std\n",
    "    x -= [0.485, 0.456, 0.406]    \n",
    "    x /= [0.229, 0.224, 0.225]     \n",
    "    \n",
    "    return x\n",
    "\n",
    "def augment_image(image):\n",
    "    image = Image.fromarray(np.uint8(image))\n",
    "    image = ImageEnhance.Brightness(image).enhance(np.random.uniform(0.8, 1.2))\n",
    "    image = ImageEnhance.Contrast(image).enhance(np.random.uniform(0.8, 1.2))\n",
    "    image = np.array(image)/255.0  # normalize\n",
    "    return image"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:02:06.121566Z",
     "iopub.execute_input": "2023-03-26T16:02:06.122581Z",
     "iopub.status.idle": "2023-03-26T16:02:06.130167Z",
     "shell.execute_reply.started": "2023-03-26T16:02:06.122541Z",
     "shell.execute_reply": "2023-03-26T16:02:06.128799Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x_col='image_path'\n",
    "y_col='label'\n",
    "batch_size=32\n",
    "seed=42\n",
    "shuffle=True\n",
    "class_mode='binary'\n",
    "target_size=(320,320)\n",
    "\n",
    "# create Data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=augment_image\n",
    ")\n",
    "\n",
    "\n",
    "valid_test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0\n",
    ")\n",
    "\n",
    "\n",
    "# prepare iterators\n",
    "train_iterator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed,\n",
    "    shuffle=shuffle,\n",
    "    class_mode='binary',\n",
    "    target_size=target_size\n",
    ")\n",
    "\n",
    "valid_iterator = valid_test_datagen.flow_from_dataframe(\n",
    "    dataframe=valid,\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    shuffle=shuffle,\n",
    "    class_mode='binary',\n",
    "    target_size=target_size\n",
    ")\n",
    "\n",
    "test_iterator = valid_test_datagen.flow_from_dataframe(\n",
    "    dataframe=test,\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    class_mode='binary',\n",
    "    target_size=target_size\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:04:55.956963Z",
     "iopub.execute_input": "2023-03-26T16:04:55.957419Z",
     "iopub.status.idle": "2023-03-26T16:07:00.041048Z",
     "shell.execute_reply.started": "2023-03-26T16:04:55.957382Z",
     "shell.execute_reply": "2023-03-26T16:07:00.039937Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "Found 31956 validated image filenames belonging to 2 classes.\nFound 4068 validated image filenames belonging to 2 classes.\nFound 3981 validated image filenames belonging to 2 classes.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow_addons as tfa\n",
    "def opt_es(learning_rate=0.0001, monitor='val_loss', patience=10) -> tuple:\n",
    "    \"\"\"return the Adam optimizer and the readly stopping\"\"\"\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    early_stopping = EarlyStopping(\n",
    "            monitor=monitor,\n",
    "            patience=patience,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    return optimizer, early_stopping, reduceLR\n",
    "\n",
    "def clean_up(model):\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "def print_eval(hs, _eval) -> None:\n",
    "    \"\"\"Prints Train, validation and test metrics for an input hs object\"\"\"\n",
    "\n",
    "    print(\"Train Loss     : {0:.5f}\".format(hs.history['loss'][-1]))\n",
    "    print(\"Validation Loss: {0:.5f}\".format(hs.history['val_loss'][-1]))\n",
    "    print(\"Test Loss      : {0:.5f}\".format(_eval[0]))\n",
    "    print(\"---\")\n",
    "    print(\"Train Accuracy     : {0:.5f}\".format(hs.history['accuracy'][-1]))\n",
    "    print(\"Validation Accuracy: {0:.5f}\".format(hs.history['val_accuracy'][-1]))\n",
    "    print(\"Test Accuracy      : {0:.5f}\".format(_eval[2]))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:07:13.567996Z",
     "iopub.execute_input": "2023-03-26T16:07:13.568755Z",
     "iopub.status.idle": "2023-03-26T16:07:13.835204Z",
     "shell.execute_reply.started": "2023-03-26T16:07:13.568713Z",
     "shell.execute_reply": "2023-03-26T16:07:13.834180Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(\n",
    "        train_iterator,\n",
    "        valid_iterator,\n",
    "        optimizer: tf.keras.optimizers,\n",
    "        filter_size: List[int],\n",
    "        kernel_size: List[int],\n",
    "        batch_norm: List[bool],\n",
    "        max_pooling_size: List[int],\n",
    "        metrics: List[Any],\n",
    "        dropout: List[Optional[float]],\n",
    "        conv_activation: str = 'relu',\n",
    "        output_activation: str = 'sigmoid',\n",
    "        callbacks: Optional[List[Any]] = None, \n",
    "        verbose: int = 0,\n",
    "        epochs: int = 20,\n",
    "        train: bool = True) -> tuple:\n",
    "    \n",
    "    \n",
    "    assert len(filter_size) == len(kernel_size) == len(batch_norm) == len(max_pooling_size) == len(dropout)\n",
    "    \n",
    "    np.random.seed(42) # Define the seed for numpy to have reproducible experiments.\n",
    "    set_seed(42) # Define the seed for Tensorflow to have reproducible experiments.\n",
    "    \n",
    "    # Define the input layer.\n",
    "    _input = Input(\n",
    "        shape=(320, 320, 3),\n",
    "        name='Input'\n",
    "    )\n",
    "\n",
    "    x = _input\n",
    "     # Define the convolutional layers.\n",
    "    for i, filters in enumerate(filter_size):\n",
    "        x = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=(kernel_size[i], kernel_size[i]),\n",
    "            strides=(1, 1),\n",
    "            padding='same',\n",
    "            dilation_rate=(1, 1),\n",
    "            activation=conv_activation,\n",
    "            name='Conv2D-{0:d}'.format(i + 1)\n",
    "        )(x)\n",
    "        \n",
    "        if batch_norm[i]:\n",
    "            x = BatchNormalization(\n",
    "                name='BatchNormalization-{0:d}'.format(i + 1)\n",
    "            )(x)\n",
    "\n",
    "        if dropout[i] > 0:\n",
    "            x = Dropout(\n",
    "                rate=dropout[i],\n",
    "                name='Dropout-{0:d}'.format(i + 1)\n",
    "            )(x)\n",
    "            \n",
    "        if max_pooling_size[i]:\n",
    "            x = MaxPool2D(\n",
    "                pool_size=(max_pooling_size[i], max_pooling_size[i]),\n",
    "                strides=(2, 2),\n",
    "                padding='same',\n",
    "                name='MaxPool2D-{0:d}'.format(i + 1)\n",
    "            )(x)\n",
    "            \n",
    "    # Flatten the convolved images so as to input them to a Dense Layer\n",
    "    x = Flatten(name='Flatten')(x)\n",
    "    x = Dense(units=64, activation='relu')(x)\n",
    "    \n",
    "    # Define the output layer.\n",
    "    output = Dense(\n",
    "        units=1,\n",
    "        activation=output_activation,\n",
    "        name='Output'\n",
    "    )(x)\n",
    "\n",
    "    # Define the model and train it.\n",
    "    model = Model(inputs=_input, outputs=output)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "    model.summary() # Print a description of the model.\n",
    "    if train:\n",
    "        hs = model.fit(\n",
    "            train_iterator,\n",
    "            validation_data=valid_iterator,\n",
    "            epochs=epochs,\n",
    "            verbose=verbose,\n",
    "            callbacks=callbacks,\n",
    "            shuffle=True\n",
    "        )\n",
    "        print('Finished training.')\n",
    "        print('------------------')\n",
    "        \n",
    "        return model, hs\n",
    "    else:\n",
    "        return model, None"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-26T16:07:15.318087Z",
     "iopub.execute_input": "2023-03-26T16:07:15.318455Z",
     "iopub.status.idle": "2023-03-26T16:07:15.335063Z",
     "shell.execute_reply.started": "2023-03-26T16:07:15.318423Z",
     "shell.execute_reply": "2023-03-26T16:07:15.333679Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "cnn_params = {\n",
    "        'filter_size': [64, 64, 32, 16],\n",
    "        'kernel_size':[3, 5, 3, 3],\n",
    "        'batch_norm': [True, True, True, True],\n",
    "        'max_pooling_size': [4, 2, 4, 2],\n",
    "        'dropout': [0.0, 0.4, 0.3, 0.6],\n",
    "        'learning_rate': [0.001],\n",
    "        'epochs': 30\n",
    "    }\n",
    "\n",
    "# init wandb\n",
    "wandb.init(project=\"Deep_Learning_2\", \n",
    "           name=\"CNN_baseline_test_small\")\n",
    "\n",
    "\n",
    "# Metrics and optimizer\n",
    "metrics = [tfa.metrics.CohenKappa(name=\"cohen_kappa\", num_classes=2),\n",
    "           'accuracy']\n",
    "optimizer, early_stopping, reduceLR = opt_es(cnn_params['learning_rate'])\n",
    "\n",
    "# train model\n",
    "model, hs = train_model(\n",
    "    train_iterator=train_iterator,\n",
    "    valid_iterator=valid_iterator,\n",
    "    optimizer=optimizer,\n",
    "    epochs=cnn_params['epochs'],\n",
    "    filter_size=cnn_params['filter_size'],\n",
    "    kernel_size=cnn_params['kernel_size'],\n",
    "    batch_norm=cnn_params['batch_norm'],\n",
    "    max_pooling_size=cnn_params['max_pooling_size'],\n",
    "    dropout=cnn_params['dropout'],\n",
    "    conv_activation='relu',\n",
    "    output_activation='sigmoid',\n",
    "    callbacks=[WandbCallback(), early_stopping, reduceLR],\n",
    "    metrics=metrics,\n",
    "    verbose=1\n",
    "\n",
    ")\n",
    "\n",
    "# evaluate\n",
    "# Evaluate on test data and show all the results.\n",
    "_eval = model.evaluate(test_iterator, verbose=1)\n",
    "print_eval(hs, _eval)\n",
    "clean_up(model=model)\n",
    "wandb.finish()\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-25T16:44:51.941413Z",
     "iopub.execute_input": "2023-03-25T16:44:51.942609Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
