{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "> Chalkiopoulos Georgios, Electrical and Computer Engineer NTUA <br />\n",
    "> Data Science postgraduate Student <br />\n",
    "> gchalkiopoulos@aueb.gr"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_22760/2783894318.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\JOJOSH~1\\AppData\\Local\\Temp/ipykernel_22760/2783894318.py\"\u001B[1;36m, line \u001B[1;32m3\u001B[0m\n\u001B[1;33m    ---\u001B[0m\n\u001B[1;37m       ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n!pip install -U selenium\\n!pip install webdriver-manager\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install -U selenium\n",
    "!pip install webdriver-manager\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common import NoSuchElementException, TimeoutException, ElementClickInterceptedException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.remote import webelement\n",
    "\n",
    "from pathlib import Path\n",
    "import csv, time\n",
    "from typing import TextIO, List, Tuple\n",
    "import logging\n",
    "import re\n",
    "from re import Pattern\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AmazonScrapper Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class AmazonScrapper:\n",
    "    \"\"\"Class that scrapes amazon reviews. Searches for the first product (user defined) and saves the rating text, the rating score and the date\"\"\"\n",
    "    website: str = \"https://www.amazon.co.uk/\"\n",
    "    logger = logging.getLogger(\"AmazonLogger\")\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 query: str,\n",
    "                 driver: WebDriver = None,\n",
    "                 output_path: str = None,\n",
    "                 wait: int = 5,\n",
    "                 max_pages: int = 300\n",
    "                 ):\n",
    "        self.query = query\n",
    "        self.driver = driver\n",
    "        self.output_path: Path = Path(f\"amazon_reviews_{query.replace(' ', '_')}.csv\") if output_path is None else output_path\n",
    "        self.wait = wait\n",
    "        self.logger = self._setup_logger()\n",
    "        self.writer, self.fw = self._writer()\n",
    "        self.max_pages = max_pages\n",
    "\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        \"\"\"Setup up logger\"\"\"\n",
    "\n",
    "        # Create logger\n",
    "        logger = logging.getLogger(self.__class__.__name__)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        if not logger.handlers:\n",
    "            # Create console handler and set level to debug\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.INFO)\n",
    "\n",
    "            # Create formatter\n",
    "            formatter = logging.Formatter('[%(asctime)s] %(levelname)s [%(name)s] - %(message)s')\n",
    "\n",
    "            # Add formatter to ch\n",
    "            ch.setFormatter(formatter)\n",
    "\n",
    "            # Add ch to logger\n",
    "            logger.addHandler(ch)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _writer(self) -> Tuple[csv.writer, TextIO]:\n",
    "        \"\"\"Initiates a csv.writer method and returns it\"\"\"\n",
    "\n",
    "        # open a new csv writer\n",
    "        fw: TextIO = self.output_path.open(mode=\"w\",encoding=\"utf8\")\n",
    "        writer = csv.writer(fw,lineterminator=\"\\n\")\n",
    "        writer.writerow([\"text\", \"rating\", \"date\"])\n",
    "        return writer, fw\n",
    "\n",
    "\n",
    "    def get_reviews(self) -> None:\n",
    "        \"\"\"Main method that performs needed steps to get the reviews\"\"\"\n",
    "\n",
    "        self._setup_driver()\n",
    "        self._load_main_page()\n",
    "        self._accept_cookies()\n",
    "        self._apply_query()\n",
    "\n",
    "        product, product_name = self._find_product()\n",
    "        self._click_product(product, product_name)\n",
    "        self._download_image()\n",
    "\n",
    "\n",
    "        self._see_all_reviews()\n",
    "        self._process_reviews()\n",
    "\n",
    "\n",
    "        self.logger.info(\"Closing Driver.\")\n",
    "        self.driver.quit()\n",
    "        self.fw.close()\n",
    "\n",
    "    def _setup_driver(self) -> WebDriver:\n",
    "        if self.driver:\n",
    "            pass\n",
    "        else:\n",
    "            self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "        return self.driver\n",
    "\n",
    "    def _load_main_page(self) -> None:\n",
    "        \"\"\"Loads main page\"\"\"\n",
    "        self.logger.info(f\"Initialize website: {self.website}.\")\n",
    "        self.driver.maximize_window()\n",
    "        self.driver.get(self.website)\n",
    "        time.sleep(self.wait)\n",
    "\n",
    "\n",
    "    def _accept_cookies(self) -> None:\n",
    "        \"\"\"Try to accept cookies\"\"\"\n",
    "        WebDriverWait(WebDriver, self.wait)\n",
    "        try:\n",
    "            accept_box = self.driver.find_element(by=By.ID, value=\"sp-cc-accept\")\n",
    "            accept_box.click()\n",
    "            self.logger.info(\"Cookies accepted\")\n",
    "        except NoSuchElementException:\n",
    "            self.logger.warning(\"Cookies element not found.\")\n",
    "        time.sleep(self.wait)\n",
    "\n",
    "\n",
    "    def _apply_query(self) -> None:\n",
    "        \"\"\"Find the search box and apply the query\"\"\"\n",
    "\n",
    "        # find search box\n",
    "        search_box = self.driver.find_element(by=By.ID, value=\"twotabsearchtextbox\")\n",
    "        search_box.send_keys(self.query)\n",
    "\n",
    "        # press search button\n",
    "        search = self.driver.find_element(by=By.ID, value=\"nav-search-submit-button\")\n",
    "        search.click()\n",
    "        self.logger.info(f\"Search for {self.query} submitted.\")\n",
    "        time.sleep(self.wait)\n",
    "\n",
    "    def _find_product(self) -> webelement:\n",
    "        \"\"\"finds the first non-sponsored product\"\"\"\n",
    "        items =  WebDriverWait(self.driver,self.wait).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"[data-component-type='s-search-result']\")))\n",
    "\n",
    "        for product in items:\n",
    "\n",
    "            # Find product name\n",
    "            try:\n",
    "                product_name = product.find_element(by=By.XPATH, value=\".//h2/a/span[contains(@class,'text')]\").text\n",
    "            except NoSuchElementException:\n",
    "                self.logger.warning(\"Class name 'a-size-base-plus a-color-base a-text-normal' (Product Name) not found.\")\n",
    "                continue\n",
    "\n",
    "            # return first non-sponsored product\n",
    "            if self._is_sponsored(product):\n",
    "                self.logger.info(f\"Skipping sponsored product: {product_name}.\")\n",
    "                continue\n",
    "            else:\n",
    "                self.logger.info(f\"Found not sponsored product: {product_name}\")\n",
    "                return product, product_name\n",
    "        time.sleep(self.wait)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_sponsored(_product) -> bool:\n",
    "        \"\"\"Checks if a listed product is a sponsored one\"\"\"\n",
    "        # skip sponsored\n",
    "        try:\n",
    "            _product.find_element(by=By.CSS_SELECTOR, value=\"[aria-label='View Sponsored information or leave ad feedback']\")\n",
    "            return True\n",
    "        except NoSuchElementException:\n",
    "            return False\n",
    "\n",
    "    def _click_product(self, product: webelement, product_name: str) -> None:\n",
    "        \"\"\"click on product\"\"\"\n",
    "        # try to find the clickable link\n",
    "        try:\n",
    "            link = product.find_element(by=By.XPATH, value=\".//h2/a[contains(@class,'a-link-normal')]\")\n",
    "            link.click()\n",
    "            self.logger.info(f\"Clicked on product: {product_name}.\")\n",
    "            time.sleep(self.wait)\n",
    "        except NoSuchElementException:\n",
    "            self.logger.error(f\"Could not find clickable link for the product: {product_name}.\")\n",
    "            raise ValueError(f\"Clickable link not found for the product {product_name}. Please check!\")\n",
    "        time.sleep(self.wait)\n",
    "\n",
    "\n",
    "    def _download_image(self) -> None:\n",
    "        \"\"\"download the first image product\"\"\"\n",
    "\n",
    "        try:\n",
    "            image = WebDriverWait(self.driver, self.wait).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[id=\"dp-container\"]')))\n",
    "            landingImageUrl: str = image.find_element(by=By.CSS_SELECTOR, value='[type=\"a-state\"]').get_attribute('innerHTML')\n",
    "\n",
    "            link: dict = json.loads(landingImageUrl)\n",
    "            response = requests.get(link.get(\"landingImageUrl\"))\n",
    "            if response.status_code:\n",
    "                fp = open(f\"{self.query.replace(' ', '_')}.png\", \"wb\")\n",
    "                fp.write(response.content)\n",
    "                fp.close()\n",
    "            self.logger.info(\"Product Image Downloaded.\")\n",
    "        except:\n",
    "            self.logger.warning(\"Could not download product image. Please download manually.\")\n",
    "\n",
    "\n",
    "\n",
    "    def _see_all_reviews(self) -> None:\n",
    "        \"\"\"Click on the see all reviews button\"\"\"\n",
    "\n",
    "        # skip sponsored\n",
    "        try:\n",
    "            local_reviews = WebDriverWait(self.driver, self.wait).until(EC.element_to_be_clickable((By.CLASS_NAME, \"cr-widget-FocalReviews\")))\n",
    "            all_reviews = local_reviews.find_element(by=By.CSS_SELECTOR, value=\"[data-hook='see-all-reviews-link-foot']\")\n",
    "            all_reviews.click()\n",
    "            self.logger.info(\"Clicked See all Reviews (Local Reviews).\")\n",
    "            time.sleep(self.wait)\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            self.logger.warning(\"Could not find Local review element.\")\n",
    "\n",
    "    def _get_page_reviews(self) -> List[webelement.WebElement]:\n",
    "        \"\"\"returns all reviews in a page\"\"\"\n",
    "\n",
    "        # scroll down\n",
    "        self.driver.execute_script('window,scrollTo(0,document.body.scrollHeight)')\n",
    "\n",
    "        # get all the reviews in the page\n",
    "        try:\n",
    "            reviews =  WebDriverWait(self.driver, self.wait).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '[data-hook=\"review\"]')))\n",
    "            self.logger.info(\"Reviews Loaded.\")\n",
    "            return reviews\n",
    "        except NoSuchElementException:\n",
    "            self.logger.warning(f\"Could not find the 'review' CSS element in data-hook.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _write_review(self, review: webelement.WebElement) -> None:\n",
    "        \"\"\"Add a line with the text, rating and date of a given review.\n",
    "        Uses code from the lecture Customer Analytics\n",
    "\n",
    "        Args:\n",
    "            review: a webelement.WebElement object with the current review\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize key attributes\n",
    "        rating, content, date ='NA','NA','NA'\n",
    "\n",
    "        # try to find the date box\n",
    "        try:\n",
    "            date_box = review.find_element(by=By.CSS_SELECTOR, value='[data-hook=\"review-date\"]')\n",
    "        except NoSuchElementException:\n",
    "            date_box = None\n",
    "\n",
    "        # box found, extract text\n",
    "        if date_box:\n",
    "            date_text: str = date_box.text\n",
    "\n",
    "        # Only keep EN reviews\n",
    "        pattern: Pattern = re.compile(\"Reviewed in (?P<country>.*) on (?P<review_date>.*)\")\n",
    "        match: dict = re.search(pattern, date_text).groupdict()\n",
    "        countries: List[str] = [\"United States\", \"Australia\", \"United Kingdom\"]\n",
    "\n",
    "        if any(x in match.get(\"country\") for x in countries):\n",
    "            date = datetime.strptime(match[\"review_date\"], '%d %B %Y').strftime(\"%Y/%m/%d\")\n",
    "            self.logger.debug(f\"English review: {match.get('country')}\")\n",
    "        else:\n",
    "            self.logger.debug(f\"Skipping Non-English review: {match.get('country')}\")\n",
    "            return\n",
    "\n",
    "\n",
    "        # try to find the rating box\n",
    "        try:\n",
    "            rating_box=review.find_element(by=By.CSS_SELECTOR, value='[data-hook*=\"review-star-rating\"]')\n",
    "        except NoSuchElementException:\n",
    "            rating_box=None\n",
    "\n",
    "        # box found\n",
    "        if rating_box:\n",
    "            rating_info=rating_box.get_attribute('class') # get the text of class attribute\n",
    "            rating = re.search('a-star-(\\d)',rating_info)  # look for the star rating from the class text\n",
    "            rating = rating.group(1) # extract the star rating\n",
    "\n",
    "        # try to find the content box\n",
    "        try:\n",
    "            review_text = review.find_element(by=By.CSS_SELECTOR, value='[data-hook=\"review-body\"]')\n",
    "        except NoSuchElementException:\n",
    "            review_text = None\n",
    "\n",
    "        # box found, extract text\n",
    "        if review_text:\n",
    "            text = review_text.text\n",
    "\n",
    "        # write a new row\n",
    "        self.writer.writerow([text, rating, date])\n",
    "\n",
    "\n",
    "    def _process_reviews(self) -> None:\n",
    "        \"\"\"Loads next review page until the end. Calls self._write_review and self._get_page_reviews\"\"\"\n",
    "\n",
    "        page: int = 1\n",
    "        while True:\n",
    "            try:\n",
    "                reviews = self._get_page_reviews()\n",
    "\n",
    "                self.logger.info(f\"Iterating Page {page}.\")\n",
    "                for review in reviews:\n",
    "\n",
    "                    try:\n",
    "                        self._write_review(review)\n",
    "                    except:\n",
    "                        self.logger.warning(\"Could not write review.\")\n",
    "                        time.sleep(self.wait)\n",
    "\n",
    "            except TimeoutException:\n",
    "                self.logger.warning(\"Could not load reviews.\")\n",
    "                self.driver.refresh()\n",
    "\n",
    "            # wait until the next Button loads\n",
    "            try:\n",
    "                next_button = WebDriverWait(self.driver,self.wait*10).until(EC.presence_of_element_located((By.CLASS_NAME,'a-last')))\n",
    "                time.sleep(self.wait)\n",
    "            except TimeoutException:\n",
    "                self.logger.warning(\"Could not Scroll. Refreshing Page, please scroll manually.\")\n",
    "                self.driver.refresh()\n",
    "                continue\n",
    "\n",
    "            # final page reached, 'next' button is disabled on this page\n",
    "            if 'a-disabled' in next_button.get_attribute('class'):\n",
    "                self.logger.info(\"Reached Last Page.\")\n",
    "                break\n",
    "\n",
    "            # stop after max_pages pages loaded\n",
    "            if page == self.max_pages:\n",
    "                self.logger.info(f\"Reached {self.max_pages} Pages. Stopping...\")\n",
    "                break\n",
    "\n",
    "            # click on the next Button\n",
    "            try:\n",
    "                next_button.click()\n",
    "\n",
    "            except ElementClickInterceptedException:\n",
    "                self.logger.warning(\"Could not Click. Refreshing Page, please scroll manually.\")\n",
    "                self.driver.refresh()\n",
    "                time.sleep(self.wait*2)\n",
    "\n",
    "            # wait for a few seconds\n",
    "            time.sleep(self.wait)\n",
    "            page += 1\n",
    "\n",
    "        self.logger.info(f\"All reviews loaded. file saved under: \\n{self.output_path.absolute()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scrape Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def scrape(query: str,\n",
    "           driver: WebDriver = None,\n",
    "           wait: int = 3,\n",
    "           output_path: str = None,\n",
    "           max_pages: int = 100) -> None:\n",
    "    \"\"\"\n",
    "    Functions that accepts a query (along with a selenium.webdriver.chrome.webdriver.Webdriver)\n",
    "    and scraps the first non sponsored product from amazon.co.uk.\n",
    "    Uses the AmazonScrapper Class\n",
    "\n",
    "    Args:\n",
    "        query: the name of the product to search\n",
    "        driver (optional): a Webdriver object\n",
    "        wait (optional): wait time. Set to 5 by default due to stable performance\n",
    "        output_path (optional): output_path name. Default in amazon_reviews_{query}.csv\n",
    "        max_pages (optional): max review pages to load. Each page contains 10 reviews\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    AmazonScrapper(query=query,\n",
    "                   driver=driver,\n",
    "                   wait=wait,\n",
    "                   output_path=output_path,\n",
    "                   max_pages=max_pages).get_reviews()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Execution Cell"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-31 19:17:58,076] INFO [AmazonScrapper] - Initialize website: https://www.amazon.co.uk/.\n",
      "[2022-12-31 19:18:03,384] INFO [AmazonScrapper] - Cookies accepted\n",
      "[2022-12-31 19:18:11,908] INFO [AmazonScrapper] - Search for Vans Ward Sneaker submitted.\n",
      "[2022-12-31 19:18:15,030] INFO [AmazonScrapper] - Found not sponsored product: Vans Men's Mn Ward Sneaker\n",
      "[2022-12-31 19:18:18,570] INFO [AmazonScrapper] - Clicked on product: Vans Men's Mn Ward Sneaker.\n",
      "[2022-12-31 19:18:24,689] INFO [AmazonScrapper] - Product Image Downloaded.\n",
      "[2022-12-31 19:18:26,250] INFO [AmazonScrapper] - Clicked See all Reviews (Local Reviews).\n",
      "[2022-12-31 19:18:29,566] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-31 19:18:29,568] INFO [AmazonScrapper] - Iterating Page 1.\n",
      "[2022-12-31 19:18:37,503] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-31 19:18:37,504] INFO [AmazonScrapper] - Iterating Page 2.\n",
      "[2022-12-31 19:18:44,513] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-31 19:18:44,514] INFO [AmazonScrapper] - Iterating Page 3.\n",
      "[2022-12-31 19:18:51,498] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-31 19:18:51,500] INFO [AmazonScrapper] - Iterating Page 4.\n",
      "[2022-12-31 19:18:58,449] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-31 19:18:58,450] INFO [AmazonScrapper] - Iterating Page 5.\n",
      "[2022-12-31 19:19:05,420] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-31 19:19:05,421] INFO [AmazonScrapper] - Iterating Page 6.\n",
      "[2022-12-31 19:19:13,066] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-31 19:19:13,068] INFO [AmazonScrapper] - Iterating Page 7.\n",
      "[2022-12-31 19:19:20,113] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-31 19:19:20,115] INFO [AmazonScrapper] - Iterating Page 8.\n",
      "[2022-12-31 19:19:27,107] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-31 19:19:27,107] INFO [AmazonScrapper] - Iterating Page 9.\n",
      "[2022-12-31 19:19:34,078] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-31 19:19:34,079] INFO [AmazonScrapper] - Iterating Page 10.\n",
      "[2022-12-31 19:19:37,931] INFO [AmazonScrapper] - Reached 10 Pages. Stopping...\n",
      "[2022-12-31 19:19:37,933] INFO [AmazonScrapper] - All reviews loaded. file saved under: \n",
      "C:\\Users\\jojoshulk\\PycharmProjects\\MscDataScience\\Advanced_Customer_Analytics\\Project_1\\amazon_reviews_Vans_Ward_Sneaker.csv\n",
      "[2022-12-31 19:19:37,936] INFO [AmazonScrapper] - Closing Driver.\n"
     ]
    }
   ],
   "source": [
    "query: str = \"Vans Ward Sneaker\"\n",
    "\n",
    "scrape(query=query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
