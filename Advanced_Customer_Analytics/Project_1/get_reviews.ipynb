{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Review Summarization\n",
    "\n",
    "1. Select an English-speaking website that hosts customer reviews on products (or services, businesses, movies, events, etc).\n",
    "\n",
    "2. Make sure that the website includes a free-text search box that users can use to search for products.\n",
    "\n",
    "3. Email me your selection at ted@aueb.gr. Each student should work on a different website, so I will maintain the list of selected websites at the top of our Wiki. First come, first served.\n",
    "\n",
    "4. Create a first Python Notebook with a function called scrape( ). The function should accept as a parameter a query (a word or short phrase).  The function should then use selenium to:\n",
    "\n",
    "   * submit the query to the website's search box and retrieve the list of matching products.\n",
    "   * access the first product on the list and download all its reviews into a csv file. For each review, the function should get the text, the rating, and the date. One line per review, 3 fields per line.\n",
    "\n",
    "5. Create a second Python Notebook with a function called summarize( ). The function should accept as a parameter the path to a csv file created by the first Notebook. It should then create a 1-page pdf file that includes a summary of all the reviews in the csv.\n",
    "\n",
    "The nature of the summary is entirely up to you. It can be text-based, visual-based, or a combination of both.\n",
    "It is also up to you to define what is important enough to be included in the summary.\n",
    "Focus on creating a summary that you think would be the most informative for customers.\n",
    "The creation of the pdf should be done through the notebook.\n",
    "You can use whatever Python-based library that you want.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> Chalkiopoulos Georgios, Electrical and Computer Engineer NTUA <br />\n",
    "> Data Science postgraduate Student <br />\n",
    "> gchalkiopoulos@aueb.gr"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_22760/2783894318.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\JOJOSH~1\\AppData\\Local\\Temp/ipykernel_22760/2783894318.py\"\u001B[1;36m, line \u001B[1;32m3\u001B[0m\n\u001B[1;33m    ---\u001B[0m\n\u001B[1;37m       ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n!pip install -U selenium\\n!pip install webdriver-manager\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!pip install -U selenium\n",
    "!pip install webdriver-manager\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.webdriver import WebDriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common import NoSuchElementException, TimeoutException, ElementClickInterceptedException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.remote import webelement\n",
    "\n",
    "from pathlib import Path\n",
    "import csv, time\n",
    "from typing import TextIO, List, Tuple\n",
    "import logging\n",
    "import re\n",
    "from re import Pattern, Match\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AmazonScrapper:\n",
    "    \"\"\"Class that scrapes amazon reviews. Searches for the first product (user defined) and saves the rating text, the rating score and the date\"\"\"\n",
    "    website: str = \"https://www.amazon.co.uk/\"\n",
    "    logger = logging.getLogger(\"AmazonLogger\")\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 query: str,\n",
    "                 driver: WebDriver = None,\n",
    "                 output_path: str = None,\n",
    "                 wait: int = 5\n",
    "                 ):\n",
    "        self.query = query\n",
    "        self.driver = driver\n",
    "        self.output_path: Path = Path(f\"amazon_reviews_{query.replace(' ', '_')}.csv\") if output_path is None else output_path\n",
    "        self.wait = wait\n",
    "        self.logger = self._setup_logger()\n",
    "        self.writer, self.fw = self._writer()\n",
    "\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        \"\"\"Setup up logger\"\"\"\n",
    "\n",
    "        # Create logger\n",
    "        logger = logging.getLogger(self.__class__.__name__)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        if not logger.handlers:\n",
    "            # Create console handler and set level to debug\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.INFO)\n",
    "\n",
    "            # Create formatter\n",
    "            formatter = logging.Formatter('[%(asctime)s] %(levelname)s [%(name)s] - %(message)s')\n",
    "\n",
    "            # Add formatter to ch\n",
    "            ch.setFormatter(formatter)\n",
    "\n",
    "            # Add ch to logger\n",
    "            logger.addHandler(ch)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    def _writer(self) -> Tuple[csv.writer, TextIO]:\n",
    "        \"\"\"Initiates a csv.writer method and returns it\"\"\"\n",
    "\n",
    "        # open a new csv writer\n",
    "        fw: TextIO = self.output_path.open(mode=\"w\",encoding=\"utf8\")\n",
    "        writer = csv.writer(fw,lineterminator=\"\\n\")\n",
    "        writer.writerow([\"text\", \"rating\", \"date\"])\n",
    "        return writer, fw\n",
    "\n",
    "\n",
    "    def get_reviews(self) -> None:\n",
    "        \"\"\"Main method that performs needed steps to get the reviews\"\"\"\n",
    "\n",
    "        self._setup_driver()\n",
    "        self._load_main_page()\n",
    "        self._accept_cookies()\n",
    "        self._apply_query()\n",
    "\n",
    "        product, product_name = self._find_product()\n",
    "        self._click_product(product, product_name)\n",
    "\n",
    "        self._see_all_reviews()\n",
    "        self._process_reviews()\n",
    "\n",
    "\n",
    "        self.logger.info(\"Closing Driver.\")\n",
    "        self.driver.quit()\n",
    "        self.fw.close()\n",
    "\n",
    "    def _setup_driver(self) -> WebDriver:\n",
    "        if self.driver:\n",
    "            pass\n",
    "        else:\n",
    "            self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "        return self.driver\n",
    "\n",
    "    def _load_main_page(self) -> None:\n",
    "        \"\"\"Loads main page\"\"\"\n",
    "        self.logger.info(f\"Initialize website: {self.website}.\")\n",
    "        self.driver.maximize_window()\n",
    "        self.driver.get(self.website)\n",
    "        time.sleep(self.wait)\n",
    "\n",
    "\n",
    "    def _accept_cookies(self) -> None:\n",
    "        \"\"\"Try to accept cookies\"\"\"\n",
    "        WebDriverWait(WebDriver, self.wait)\n",
    "        try:\n",
    "            accept_box = self.driver.find_element(by=By.ID, value=\"sp-cc-accept\")\n",
    "            accept_box.click()\n",
    "            self.logger.info(\"Cookies accepted\")\n",
    "        except NoSuchElementException:\n",
    "            self.logger.warning(\"Cookies element not found.\")\n",
    "        time.sleep(self.wait)\n",
    "\n",
    "\n",
    "    def _apply_query(self) -> None:\n",
    "        \"\"\"Find the search box and apply the query\"\"\"\n",
    "\n",
    "        # find search box\n",
    "        search_box = self.driver.find_element(by=By.ID, value=\"twotabsearchtextbox\")\n",
    "        search_box.send_keys(self.query)\n",
    "\n",
    "        # press search button\n",
    "        search = self.driver.find_element(by=By.ID, value=\"nav-search-submit-button\")\n",
    "        search.click()\n",
    "        self.logger.info(f\"Search for {self.query} submitted.\")\n",
    "        time.sleep(self.wait)\n",
    "\n",
    "    def _find_product(self) -> webelement:\n",
    "        \"\"\"finds the first non-sponsored product\"\"\"\n",
    "        items =  WebDriverWait(self.driver,self.wait).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"[data-component-type='s-search-result']\")))\n",
    "\n",
    "        for product in items:\n",
    "\n",
    "            # Find product name\n",
    "            try:\n",
    "                product_name = product.find_element(by=By.XPATH, value=\".//h2/a/span[contains(@class,'text')]\").text\n",
    "            except NoSuchElementException:\n",
    "                self.logger.warning(\"Class name 'a-size-base-plus a-color-base a-text-normal' (Product Name) not found.\")\n",
    "                continue\n",
    "\n",
    "            # return first non-sponsored product\n",
    "            if self._is_sponsored(product):\n",
    "                self.logger.info(f\"Skipping sponsored product: {product_name}.\")\n",
    "                continue\n",
    "            else:\n",
    "                self.logger.info(f\"Found not sponsored product: {product_name}\")\n",
    "                return product, product_name\n",
    "        time.sleep(self.wait)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_sponsored(_product) -> bool:\n",
    "        \"\"\"Checks if a listed product is a sponsored one\"\"\"\n",
    "        # skip sponsored\n",
    "        try:\n",
    "            _product.find_element(by=By.CSS_SELECTOR, value=\"[aria-label='View Sponsored information or leave ad feedback']\")\n",
    "            return True\n",
    "        except NoSuchElementException:\n",
    "            return False\n",
    "\n",
    "    def _click_product(self, product: webelement, product_name: str) -> None:\n",
    "        \"\"\"click on product\"\"\"\n",
    "        # try to find the clickable link\n",
    "        try:\n",
    "            link = product.find_element(by=By.XPATH, value=\".//h2/a[contains(@class,'a-link-normal')]\")\n",
    "            link.click()\n",
    "            self.logger.info(f\"Clicked on product: {product_name}.\")\n",
    "            time.sleep(self.wait)\n",
    "        except NoSuchElementException:\n",
    "            self.logger.error(f\"Could not find clickable link for the product: {product_name}.\")\n",
    "            raise ValueError(f\"Clickable link not found for the product {product_name}. Please check!\")\n",
    "        time.sleep(self.wait)\n",
    "\n",
    "\n",
    "    def _see_all_reviews(self) -> None:\n",
    "        \"\"\"Click on the see all reviews button\"\"\"\n",
    "\n",
    "        # skip sponsored\n",
    "        try:\n",
    "            local_reviews = WebDriverWait(self.driver, self.wait).until(EC.element_to_be_clickable((By.CLASS_NAME, \"cr-widget-FocalReviews\")))\n",
    "            all_reviews = local_reviews.find_element(by=By.CSS_SELECTOR, value=\"[data-hook='see-all-reviews-link-foot']\")\n",
    "            all_reviews.click()\n",
    "            self.logger.info(\"Clicked See all Reviews (Local Reviews).\")\n",
    "            time.sleep(self.wait)\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            self.logger.warning(\"Could not find Local review element.\")\n",
    "\n",
    "    def _get_page_reviews(self) -> List[webelement.WebElement]:\n",
    "        \"\"\"returns all reviews in a page\"\"\"\n",
    "\n",
    "        # scroll down\n",
    "        self.driver.execute_script('window,scrollTo(0,document.body.scrollHeight)')\n",
    "\n",
    "        # get all the reviews in the page\n",
    "        try:\n",
    "            reviews =  WebDriverWait(self.driver, self.wait).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '[data-hook=\"review\"]')))\n",
    "            self.logger.info(\"Reviews Loaded.\")\n",
    "            return reviews\n",
    "        except NoSuchElementException:\n",
    "            self.logger.warning(f\"Could not find the 'review' CSS element in data-hook.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _write_review(self, review: webelement.WebElement) -> None:\n",
    "        \"\"\"Add a line with the text, rating and date of a given review.\n",
    "        Uses code from the lecture Customer Analytics\n",
    "\n",
    "        Args:\n",
    "            review: a webelement.WebElement object with the current review\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize key attributes\n",
    "        rating, content, date ='NA','NA','NA'\n",
    "\n",
    "        # try to find the date box\n",
    "        try:\n",
    "            date_box = review.find_element(by=By.CSS_SELECTOR, value='[data-hook=\"review-date\"]')\n",
    "        except NoSuchElementException:\n",
    "            date_box = None\n",
    "\n",
    "        # box found, extract text\n",
    "        if date_box:\n",
    "            date_text: str = date_box.text\n",
    "\n",
    "        # Only keep EN reviews\n",
    "        pattern: Pattern = re.compile(\"Reviewed in (?P<country>.*) on (?P<review_date>.*)\")\n",
    "        match: dict = re.search(pattern, date_text).groupdict()\n",
    "        countries: List[str] = [\"United States\", \"Australia\", \"United Kingdom\"]\n",
    "\n",
    "        if any(x in match.get(\"country\") for x in countries):\n",
    "            date = datetime.strptime(match[\"review_date\"], '%d %B %Y').strftime(\"%Y/%m/%d\")\n",
    "        else:\n",
    "            self.logger.debug(f\"Skipping Non-English review: {match.get('country')}\")\n",
    "            return\n",
    "\n",
    "\n",
    "        # try to find the rating box\n",
    "        try:\n",
    "            rating_box=review.find_element(by=By.CSS_SELECTOR, value='[data-hook*=\"review-star-rating\"]')\n",
    "        except NoSuchElementException:\n",
    "            rating_box=None\n",
    "\n",
    "        # box found\n",
    "        if rating_box:\n",
    "            rating_info=rating_box.get_attribute('class') # get the text of class attribute\n",
    "            rating = re.search('a-star-(\\d)',rating_info)  # look for the star rating from the class text\n",
    "            rating = rating.group(1) # extract the star rating\n",
    "\n",
    "        # try to find the content box\n",
    "        try:\n",
    "            review_text = review.find_element(by=By.CSS_SELECTOR, value='[data-hook=\"review-body\"]')\n",
    "        except NoSuchElementException:\n",
    "            review_text = None\n",
    "\n",
    "        # box found, extract text\n",
    "        if review_text:\n",
    "            text = review_text.text\n",
    "\n",
    "        # write a new row\n",
    "        self.writer.writerow([text, rating, date])\n",
    "\n",
    "\n",
    "    def _process_reviews(self) -> None:\n",
    "        \"\"\"Loads next review page until the end. Calls self._write_review and self._get_page_reviews\"\"\"\n",
    "\n",
    "        page: int = 1\n",
    "        while True:\n",
    "            try:\n",
    "                reviews = self._get_page_reviews()\n",
    "\n",
    "                self.logger.info(f\"Iterating Page {page}.\")\n",
    "                for review in reviews:\n",
    "\n",
    "                    try:\n",
    "                        self._write_review(review)\n",
    "                    except:\n",
    "                        self.logger.warning(\"Could not write review.\")\n",
    "                        time.sleep(self.wait)\n",
    "\n",
    "            except TimeoutException:\n",
    "                self.logger.warning(\"Could not load reviews.\")\n",
    "\n",
    "            # wait until the next Button loads\n",
    "            next_button = WebDriverWait(self.driver,self.wait*10).until(EC.presence_of_element_located((By.CLASS_NAME,'a-last')))\n",
    "\n",
    "            # final page reached, 'next' button is disabled on this page\n",
    "            if 'a-disabled' in next_button.get_attribute('class'):\n",
    "                self.logger.info(\"Reached Last Page.\")\n",
    "                break\n",
    "\n",
    "            # stop after 100 pages loaded\n",
    "            if page == 100:\n",
    "                self.logger.info(\"Reached 150 Pages.\")\n",
    "                break\n",
    "\n",
    "            # click on the next Button\n",
    "            try:\n",
    "                next_button.click()\n",
    "\n",
    "            except ElementClickInterceptedException:\n",
    "                self.logger.warning(\"Could not Click. Refreshing Page, please scroll manually.\")\n",
    "                self.driver.refresh()\n",
    "                time.sleep(self.wait*2)\n",
    "\n",
    "            # wait for a few seconds\n",
    "            time.sleep(self.wait*2)\n",
    "            page += 1\n",
    "\n",
    "        self.logger.info(f\"All reviews loaded. file saved under: \\n{self.output_path.absolute()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def scrape(query: str,\n",
    "           driver: WebDriver = None,\n",
    "           wait: int = 5,\n",
    "           output_path: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Functions that accepts a query (along with a selenium.webdriver.chrome.webdriver.Webdriver)\n",
    "    and scraps the first non sponsored product from amazon.co.uk.\n",
    "    Uses the AmazonScrapper Class\n",
    "\n",
    "    Args:\n",
    "        query: the name of the product to search\n",
    "        driver (optional): a Webdriver object\n",
    "        wait (optional): wait time. Set to 5 by default due to stable performance\n",
    "        output_path (optional): output_path name. Default in amazon_reviews_{query}.csv\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    AmazonScrapper(query=query, driver=driver, wait=wait, output_path=output_path).get_reviews()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-26 21:17:56,069] INFO [AmazonScrapper] - Initialize website: https://www.amazon.co.uk/.\n",
      "[2022-12-26 21:18:03,155] INFO [AmazonScrapper] - Cookies accepted\n",
      "[2022-12-26 21:18:10,867] INFO [AmazonScrapper] - Search for Vans Ward Sneaker submitted.\n",
      "[2022-12-26 21:18:15,967] INFO [AmazonScrapper] - Found not sponsored product: Vans Men's Ward Sneaker\n",
      "[2022-12-26 21:18:19,687] INFO [AmazonScrapper] - Clicked on product: Vans Men's Ward Sneaker.\n",
      "[2022-12-26 21:18:30,870] INFO [AmazonScrapper] - Clicked See all Reviews (Local Reviews).\n",
      "[2022-12-26 21:18:35,938] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:18:35,939] INFO [AmazonScrapper] - Iterating Page 1.\n",
      "[2022-12-26 21:18:41,852] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:18:41,854] INFO [AmazonScrapper] - Iterating Page 2.\n",
      "[2022-12-26 21:18:47,734] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:18:47,735] INFO [AmazonScrapper] - Iterating Page 3.\n",
      "[2022-12-26 21:18:53,657] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:18:53,658] INFO [AmazonScrapper] - Iterating Page 4.\n",
      "[2022-12-26 21:18:59,664] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:18:59,665] INFO [AmazonScrapper] - Iterating Page 5.\n",
      "[2022-12-26 21:19:05,660] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:19:05,660] INFO [AmazonScrapper] - Iterating Page 6.\n",
      "[2022-12-26 21:19:11,630] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:19:11,631] INFO [AmazonScrapper] - Iterating Page 7.\n",
      "[2022-12-26 21:19:17,597] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:19:17,598] INFO [AmazonScrapper] - Iterating Page 8.\n",
      "[2022-12-26 21:19:23,580] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:19:23,581] INFO [AmazonScrapper] - Iterating Page 9.\n",
      "[2022-12-26 21:19:29,550] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:19:29,551] INFO [AmazonScrapper] - Iterating Page 10.\n",
      "[2022-12-26 21:19:35,510] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:19:35,511] INFO [AmazonScrapper] - Iterating Page 11.\n",
      "[2022-12-26 21:19:37,116] WARNING [AmazonScrapper] - Could not Click. Refreshing Page, please scroll manually.\n",
      "[2022-12-26 21:20:01,414] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:20:01,415] INFO [AmazonScrapper] - Iterating Page 11.\n",
      "[2022-12-26 21:20:07,472] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:20:07,473] INFO [AmazonScrapper] - Iterating Page 12.\n",
      "[2022-12-26 21:20:13,343] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:20:13,345] INFO [AmazonScrapper] - Iterating Page 13.\n",
      "[2022-12-26 21:20:19,258] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:20:19,260] INFO [AmazonScrapper] - Iterating Page 14.\n",
      "[2022-12-26 21:20:25,163] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:20:25,164] INFO [AmazonScrapper] - Iterating Page 15.\n",
      "[2022-12-26 21:20:26,833] WARNING [AmazonScrapper] - Could not Click. Refreshing Page, please scroll manually.\n",
      "[2022-12-26 21:20:51,252] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:20:51,253] INFO [AmazonScrapper] - Iterating Page 15.\n",
      "[2022-12-26 21:20:57,315] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:20:57,316] INFO [AmazonScrapper] - Iterating Page 16.\n",
      "[2022-12-26 21:21:03,186] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:21:03,187] INFO [AmazonScrapper] - Iterating Page 17.\n",
      "[2022-12-26 21:21:04,795] WARNING [AmazonScrapper] - Could not Click. Refreshing Page, please scroll manually.\n",
      "[2022-12-26 21:21:29,265] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:21:29,265] INFO [AmazonScrapper] - Iterating Page 17.\n",
      "[2022-12-26 21:21:35,300] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:21:35,301] INFO [AmazonScrapper] - Iterating Page 18.\n",
      "[2022-12-26 21:21:36,893] WARNING [AmazonScrapper] - Could not Click. Refreshing Page, please scroll manually.\n",
      "[2022-12-26 21:22:01,121] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:22:01,122] INFO [AmazonScrapper] - Iterating Page 18.\n",
      "[2022-12-26 21:22:07,163] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:22:07,164] INFO [AmazonScrapper] - Iterating Page 19.\n",
      "[2022-12-26 21:22:13,103] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:22:13,104] INFO [AmazonScrapper] - Iterating Page 20.\n",
      "[2022-12-26 21:22:19,133] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:22:19,134] INFO [AmazonScrapper] - Iterating Page 21.\n",
      "[2022-12-26 21:22:25,109] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:22:25,110] INFO [AmazonScrapper] - Iterating Page 22.\n",
      "[2022-12-26 21:22:31,101] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:22:31,101] INFO [AmazonScrapper] - Iterating Page 23.\n",
      "[2022-12-26 21:22:37,095] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:22:37,096] INFO [AmazonScrapper] - Iterating Page 24.\n",
      "[2022-12-26 21:22:43,137] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:22:43,138] INFO [AmazonScrapper] - Iterating Page 25.\n",
      "[2022-12-26 21:22:49,243] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:22:49,244] INFO [AmazonScrapper] - Iterating Page 26.\n",
      "[2022-12-26 21:22:55,581] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:22:55,581] INFO [AmazonScrapper] - Iterating Page 27.\n",
      "[2022-12-26 21:23:01,547] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:23:01,548] INFO [AmazonScrapper] - Iterating Page 28.\n",
      "[2022-12-26 21:23:03,100] WARNING [AmazonScrapper] - Could not Click. Refreshing Page, please scroll manually.\n",
      "[2022-12-26 21:23:27,490] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:23:27,490] INFO [AmazonScrapper] - Iterating Page 28.\n",
      "[2022-12-26 21:23:33,592] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:23:33,592] INFO [AmazonScrapper] - Iterating Page 29.\n",
      "[2022-12-26 21:23:39,489] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:23:39,490] INFO [AmazonScrapper] - Iterating Page 30.\n",
      "[2022-12-26 21:23:45,449] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:23:45,449] INFO [AmazonScrapper] - Iterating Page 31.\n",
      "[2022-12-26 21:23:51,908] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:23:51,909] INFO [AmazonScrapper] - Iterating Page 32.\n",
      "[2022-12-26 21:23:57,907] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:23:57,907] INFO [AmazonScrapper] - Iterating Page 33.\n",
      "[2022-12-26 21:24:03,875] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:24:03,876] INFO [AmazonScrapper] - Iterating Page 34.\n",
      "[2022-12-26 21:24:09,841] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:24:09,841] INFO [AmazonScrapper] - Iterating Page 35.\n",
      "[2022-12-26 21:24:11,438] WARNING [AmazonScrapper] - Could not Click. Refreshing Page, please scroll manually.\n",
      "[2022-12-26 21:24:36,035] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:24:36,035] INFO [AmazonScrapper] - Iterating Page 35.\n",
      "[2022-12-26 21:24:42,418] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:24:42,419] INFO [AmazonScrapper] - Iterating Page 36.\n",
      "[2022-12-26 21:24:43,967] WARNING [AmazonScrapper] - Could not Click. Refreshing Page, please scroll manually.\n",
      "[2022-12-26 21:25:08,088] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:25:08,089] INFO [AmazonScrapper] - Iterating Page 36.\n",
      "[2022-12-26 21:25:14,178] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:25:14,179] INFO [AmazonScrapper] - Iterating Page 37.\n",
      "[2022-12-26 21:25:20,074] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:25:20,075] INFO [AmazonScrapper] - Iterating Page 38.\n",
      "[2022-12-26 21:25:25,951] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:25:25,951] INFO [AmazonScrapper] - Iterating Page 39.\n",
      "[2022-12-26 21:25:31,962] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:25:31,963] INFO [AmazonScrapper] - Iterating Page 40.\n",
      "[2022-12-26 21:25:38,033] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:25:38,033] INFO [AmazonScrapper] - Iterating Page 41.\n",
      "[2022-12-26 21:25:43,976] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:25:43,976] INFO [AmazonScrapper] - Iterating Page 42.\n",
      "[2022-12-26 21:25:49,855] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:25:49,856] INFO [AmazonScrapper] - Iterating Page 43.\n",
      "[2022-12-26 21:25:55,810] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:25:55,811] INFO [AmazonScrapper] - Iterating Page 44.\n",
      "[2022-12-26 21:26:01,753] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:26:01,754] INFO [AmazonScrapper] - Iterating Page 45.\n",
      "[2022-12-26 21:26:07,684] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:26:07,685] INFO [AmazonScrapper] - Iterating Page 46.\n",
      "[2022-12-26 21:26:13,603] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:26:13,604] INFO [AmazonScrapper] - Iterating Page 47.\n",
      "[2022-12-26 21:26:19,559] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:26:19,559] INFO [AmazonScrapper] - Iterating Page 48.\n",
      "[2022-12-26 21:26:25,562] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:26:25,563] INFO [AmazonScrapper] - Iterating Page 49.\n",
      "[2022-12-26 21:26:31,530] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:26:31,531] INFO [AmazonScrapper] - Iterating Page 50.\n",
      "[2022-12-26 21:26:37,529] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:26:37,530] INFO [AmazonScrapper] - Iterating Page 51.\n",
      "[2022-12-26 21:26:43,481] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:26:43,482] INFO [AmazonScrapper] - Iterating Page 52.\n",
      "[2022-12-26 21:26:49,397] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:26:49,398] INFO [AmazonScrapper] - Iterating Page 53.\n",
      "[2022-12-26 21:26:55,348] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:26:55,349] INFO [AmazonScrapper] - Iterating Page 54.\n",
      "[2022-12-26 21:27:01,294] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:01,295] INFO [AmazonScrapper] - Iterating Page 55.\n",
      "[2022-12-26 21:27:07,320] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:07,320] INFO [AmazonScrapper] - Iterating Page 56.\n",
      "[2022-12-26 21:27:13,255] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:13,255] INFO [AmazonScrapper] - Iterating Page 57.\n",
      "[2022-12-26 21:27:19,205] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:19,206] INFO [AmazonScrapper] - Iterating Page 58.\n",
      "[2022-12-26 21:27:25,150] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:25,151] INFO [AmazonScrapper] - Iterating Page 59.\n",
      "[2022-12-26 21:27:31,113] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:31,114] INFO [AmazonScrapper] - Iterating Page 60.\n",
      "[2022-12-26 21:27:37,065] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:37,066] INFO [AmazonScrapper] - Iterating Page 61.\n",
      "[2022-12-26 21:27:42,527] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:42,528] INFO [AmazonScrapper] - Iterating Page 62.\n",
      "[2022-12-26 21:27:47,940] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:47,940] INFO [AmazonScrapper] - Iterating Page 63.\n",
      "[2022-12-26 21:27:53,434] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:53,435] INFO [AmazonScrapper] - Iterating Page 64.\n",
      "[2022-12-26 21:27:58,881] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:27:58,882] INFO [AmazonScrapper] - Iterating Page 65.\n",
      "[2022-12-26 21:28:04,423] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:28:04,424] INFO [AmazonScrapper] - Iterating Page 66.\n",
      "[2022-12-26 21:28:09,839] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:28:09,840] INFO [AmazonScrapper] - Iterating Page 67.\n",
      "[2022-12-26 21:28:15,266] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:28:15,267] INFO [AmazonScrapper] - Iterating Page 68.\n",
      "[2022-12-26 21:28:20,695] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:28:20,696] INFO [AmazonScrapper] - Iterating Page 69.\n",
      "[2022-12-26 21:28:26,383] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:28:26,384] INFO [AmazonScrapper] - Iterating Page 70.\n",
      "[2022-12-26 21:28:32,026] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:28:32,027] INFO [AmazonScrapper] - Iterating Page 71.\n",
      "[2022-12-26 21:28:37,605] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:28:37,606] INFO [AmazonScrapper] - Iterating Page 72.\n",
      "[2022-12-26 21:28:43,204] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:28:43,205] INFO [AmazonScrapper] - Iterating Page 73.\n",
      "[2022-12-26 21:28:48,811] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:28:48,812] INFO [AmazonScrapper] - Iterating Page 74.\n",
      "[2022-12-26 21:28:54,420] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:28:54,421] INFO [AmazonScrapper] - Iterating Page 75.\n",
      "[2022-12-26 21:29:00,012] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:29:00,013] INFO [AmazonScrapper] - Iterating Page 76.\n",
      "[2022-12-26 21:29:05,706] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:29:05,707] INFO [AmazonScrapper] - Iterating Page 77.\n",
      "[2022-12-26 21:29:11,322] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:29:11,323] INFO [AmazonScrapper] - Iterating Page 78.\n",
      "[2022-12-26 21:29:16,936] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:29:16,937] INFO [AmazonScrapper] - Iterating Page 79.\n",
      "[2022-12-26 21:29:22,613] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:29:22,614] INFO [AmazonScrapper] - Iterating Page 80.\n",
      "[2022-12-26 21:29:53,066] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:29:53,067] INFO [AmazonScrapper] - Iterating Page 81.\n",
      "[2022-12-26 21:29:58,705] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:29:58,705] INFO [AmazonScrapper] - Iterating Page 82.\n",
      "[2022-12-26 21:30:04,328] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:30:04,329] INFO [AmazonScrapper] - Iterating Page 83.\n",
      "[2022-12-26 21:30:09,959] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:30:09,960] INFO [AmazonScrapper] - Iterating Page 84.\n",
      "[2022-12-26 21:30:15,606] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:30:15,607] INFO [AmazonScrapper] - Iterating Page 85.\n",
      "[2022-12-26 21:30:46,181] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:30:46,182] INFO [AmazonScrapper] - Iterating Page 86.\n",
      "[2022-12-26 21:30:51,824] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:30:51,825] INFO [AmazonScrapper] - Iterating Page 87.\n",
      "[2022-12-26 21:31:22,640] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:31:22,640] INFO [AmazonScrapper] - Iterating Page 88.\n",
      "[2022-12-26 21:31:28,386] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:31:28,387] INFO [AmazonScrapper] - Iterating Page 89.\n",
      "[2022-12-26 21:31:34,197] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:31:34,198] INFO [AmazonScrapper] - Iterating Page 90.\n",
      "[2022-12-26 21:31:39,965] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:31:39,966] INFO [AmazonScrapper] - Iterating Page 91.\n",
      "[2022-12-26 21:31:45,885] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:31:45,886] INFO [AmazonScrapper] - Iterating Page 92.\n",
      "[2022-12-26 21:31:51,687] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:31:51,688] INFO [AmazonScrapper] - Iterating Page 93.\n",
      "[2022-12-26 21:31:57,621] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:31:57,622] INFO [AmazonScrapper] - Iterating Page 94.\n",
      "[2022-12-26 21:32:03,487] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:32:03,488] INFO [AmazonScrapper] - Iterating Page 95.\n",
      "[2022-12-26 21:32:09,405] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:32:09,406] INFO [AmazonScrapper] - Iterating Page 96.\n",
      "[2022-12-26 21:32:15,190] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:32:15,190] INFO [AmazonScrapper] - Iterating Page 97.\n",
      "[2022-12-26 21:32:21,029] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:32:21,030] INFO [AmazonScrapper] - Iterating Page 98.\n",
      "[2022-12-26 21:32:26,840] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:32:26,841] INFO [AmazonScrapper] - Iterating Page 99.\n",
      "[2022-12-26 21:32:32,669] INFO [AmazonScrapper] - Reviews Loaded.\n",
      "[2022-12-26 21:32:32,670] INFO [AmazonScrapper] - Iterating Page 100.\n",
      "[2022-12-26 21:32:33,287] INFO [AmazonScrapper] - Reached 150 Pages.\n",
      "[2022-12-26 21:32:33,288] INFO [AmazonScrapper] - All reviews loaded. file saved under: \n",
      "C:\\Users\\jojoshulk\\PycharmProjects\\MscDataScience\\Advanced_Customer_Analytics\\Project_1\\amazon_reviews_Vans_Ward_Sneaker.csv\n",
      "[2022-12-26 21:32:33,289] INFO [AmazonScrapper] - Closing Driver.\n"
     ]
    }
   ],
   "source": [
    "query: str = \"Vans Ward Sneaker\"\n",
    "\n",
    "scrape(query=query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
